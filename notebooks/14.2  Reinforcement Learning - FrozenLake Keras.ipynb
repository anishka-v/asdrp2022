{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout, Reshape\n",
    "from keras.layers import Embedding, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import Policy, BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff70c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'FrozenLake-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10df4cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecayEpsGreedyQPolicy(Policy):\n",
    "\n",
    "    def __init__(self, max_eps=.1, min_eps=.05, lamb=0.001):\n",
    "        super(DecayEpsGreedyQPolicy, self).__init__()\n",
    "        self.max_eps = max_eps\n",
    "        self.lambd = lamb\n",
    "        self._steps = 0\n",
    "        self.min_eps = min_eps\n",
    "\n",
    "    def select_action(self, q_values):\n",
    "        assert q_values.ndim == 1\n",
    "        nb_actions = q_values.shape[0]\n",
    "        eps = self.min_eps + (self.max_eps - self.min_eps) * \\\n",
    "            np.exp(-self.lambd * self._steps)\n",
    "        self._steps += 1\n",
    "        if self._steps % 1e3 == 0:\n",
    "            print(\"Current eps:\", eps)\n",
    "        if np.random.uniform() < eps:\n",
    "            action = np.random.random_integers(0, nb_actions - 1)\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a8358",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
    "env = gym.make(ENV_NAME, desc=desc, map_name=\"4x4\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b24d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_model(action_space_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(16, 4, input_length=1))\n",
    "    model.add(Reshape((4,)))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29147336",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_keras_model(nb_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad71af",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "policy = DecayEpsGreedyQPolicy(max_eps=0.9, min_eps=0, lamb=1 / (1e4))\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions,\n",
    "               memory=memory, nb_steps_warmup=500,\n",
    "               target_model_update=1e-2, policy=policy,\n",
    "               enable_double_dqn=False, batch_size=512\n",
    "               )\n",
    "dqn.compile(Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb8c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dqn.load_weights('dqn_{}_weights.h5f'.format(ENV_NAME))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d090499",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.fit(env, nb_steps=2e4, visualize=False, verbose=1, log_interval=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a35c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights(f\"dqn_{ENV_NAME}_weights.h5f\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc55909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=20, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    clear_output(wait=True)\n",
    "    action = dqn.forward(state)  # and chose action from the Q-Table\n",
    "    state, reward, done, info = env.step(action) # Finally perform the action\n",
    "    env.render()\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "env.close()\n",
    "print(f\"Game over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b45675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
