{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa51b318",
   "metadata": {},
   "source": [
    "# GloVe: Global Vectors for Word Representation\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "Source: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6574d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Import module to split the datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import modules to evaluate the metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e765040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "#root folder\n",
    "root_folder='.'\n",
    "data_folder_name='../data'\n",
    "glove_filename='glove.6B.50d.txt'\n",
    "\n",
    "train_filename='train.csv'\n",
    "# Variable for data directory\n",
    "DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n",
    "glove_path = os.path.abspath(os.path.join(DATA_PATH, glove_filename))\n",
    "\n",
    "# Both train and test set are in the root data directory\n",
    "train_path = DATA_PATH\n",
    "test_path = DATA_PATH\n",
    "\n",
    "#Relevant columns\n",
    "TEXT_COLUMN = 'text'\n",
    "TARGET_COLUMN = 'target'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4e47bb",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Recall that Word2Vec was introduced by Mikolav et al.'s 2013 paper.  This algorithm first represents any word in a training corpus by a vector (a list of numbers) -- perhaps with one-hot encoding. initialy, each of these vectors can be completely random.  Any 2 similar (in semantics) words should have word vectors that are \"close\" together.\n",
    "\n",
    "We move the vector representations of similar words closer together by maximizing the predicted probability of the two words co-occurring.  As we do this over and over again, vectors for words that are often together will end up close, and vectors for words that are rearely together will end up far apart.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b85b1f",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus .  It was created by Pennington et al. in [Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/).  \n",
    "\n",
    "The approach relies on constructing a global co-occurrence matrix of words in the corpus. It has a few steps:\n",
    "\n",
    "- During training, record the word co-occurrences of all words using a moving context window\n",
    "- Initialize each word vector randomly (same as word2vec)\n",
    "- If any two words co-occurr more frequently than is justified by their frequency in the corpus, draw their vectors together. If they co-occurr less frequently, push their vectors apart.\n",
    "    - As in word2vec, as we do this over and over the vectors of similar words will be drawn together.\n",
    "    \n",
    "    \n",
    "## Loading a pre-trained word embedding\n",
    "\n",
    "Gensim launched its own dataset storage, committed to long-term support, a sane standardized usage API and focused on datasets for unstructured text processing (no images or audio). This [Gensim-data repository](https://github.com/RaRe-Technologies/gensim-data) serves as that storage.\n",
    "\n",
    "To use, simply install Gensim and use its download API. It will \"talk\" to this repository automagically.\n",
    "\n",
    "\n",
    "More details on default downloadable Gensim data models can be found [here](https://github.com/RaRe-Technologies/gensim-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310170b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========---------------------------------------] 22.4% 23.4/104.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================------------------------] 53.2% 55.7/104.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('dog', 0.9590820074081421),\n",
       " ('monkey', 0.9203579425811768),\n",
       " ('bear', 0.9143137335777283),\n",
       " ('pet', 0.9108031392097473),\n",
       " ('girl', 0.8880630731582642),\n",
       " ('horse', 0.8872725963592529),\n",
       " ('kitty', 0.8870542049407959),\n",
       " ('puppy', 0.8867696523666382),\n",
       " ('hot', 0.886525571346283),\n",
       " ('lady', 0.8845519423484802)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the default gensim Glove models\n",
    "\n",
    "import gensim.downloader\n",
    "\n",
    "model = gensim.downloader.load(\"glove-twitter-25\") \n",
    "\n",
    "model.most_similar(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8804fb17",
   "metadata": {},
   "source": [
    "## Using Downloaded GloVe Model\n",
    "\n",
    "Files with the pre-trained vectors Glove can be found in many sites like Kaggle or in the previous link of the Stanford University. We will use the glove.6B.100d.txt file containing the glove vectors trained on the Wikipedia and GigaWord dataset.\n",
    "\n",
    "First we convert the GloVe file containing the word embeddings to the word2vec format for convenience of use. We can do it using the gensim library, a function called glove2word2vec.\n",
    "\n",
    "If we have already downloaded a GloVe model, here is how to load it.\n",
    "\n",
    "First, we need to run this following code once.  he function glove2word2vec saves the Glove embeddings in the word2vec format that will be loaded next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d29f9b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a6235fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vz/cgd1p3b96r976sjkzw9m_7840000gp/T/ipykernel_34536/571975213.py:3: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_path, word2vec_output_file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#glove_input_file = glove_filename\n",
    "word2vec_output_file = glove_filename+'.word2vec'\n",
    "glove2word2vec(glove_path, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2670d3",
   "metadata": {},
   "source": [
    "So our vocabulary contains 400K words represented by a feature vector of shape 100. Now we can load the Glove embeddings in word2vec format and then analyze some analogies. In this way if we want to use a pre-trained word2vec embeddings we can simply change the filename and reuse all the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "170aa618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King:  [-0.32307  -0.87616   0.21977   0.25268   0.22976   0.7388   -0.37954\n",
      " -0.35307  -0.84369  -1.1113   -0.30266   0.33178  -0.25113   0.30448\n",
      " -0.077491 -0.89815   0.092496 -1.1407   -0.58324   0.66869  -0.23122\n",
      " -0.95855   0.28262  -0.078848  0.75315   0.26584   0.3422   -0.33949\n",
      "  0.95608   0.065641  0.45747   0.39835   0.57965   0.39267  -0.21851\n",
      "  0.58795  -0.55999   0.63368  -0.043983 -0.68731  -0.37841   0.38026\n",
      "  0.61641  -0.88269  -0.12346  -0.37928  -0.38318   0.23868   0.6685\n",
      " -0.43321  -0.11065   0.081723  1.1569    0.78958  -0.21223  -2.3211\n",
      " -0.67806   0.44561   0.65707   0.1045    0.46217   0.19912   0.25802\n",
      "  0.057194  0.53443  -0.43133  -0.34311   0.59789  -0.58417   0.068995\n",
      "  0.23944  -0.85181   0.30379  -0.34177  -0.25746  -0.031101 -0.16285\n",
      "  0.45169  -0.91627   0.64521   0.73281  -0.22752   0.30226   0.044801\n",
      " -0.83741   0.55006  -0.52506  -1.7357    0.4751   -0.70487   0.056939\n",
      " -0.7132    0.089623  0.41394  -1.3363   -0.61915  -0.33089  -0.52881\n",
      "  0.16483  -0.98878 ]\n",
      "Most similar word to King + Woman:  [('queen', 0.7698541283607483)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "word2vec_output_file = glove_filename+'.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "\n",
    "#Show a word embedding\n",
    "print('King: ',model.get_vector('king'))\n",
    "\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "\n",
    "print('Most similar word to King + Woman: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b8670",
   "metadata": {},
   "source": [
    "## Analyzing the vector space and find analogies\n",
    "\n",
    "We would like extract some interesting features of our word embeddings,Now, our words are numerical vectors so we can measure and compare distances between words to show some of the properties that these embedding provide.\n",
    "\n",
    "For example, we can compare some analogies. The most famous is the following: king – man + woman = queen. In other words, adding the vectors associated with the words king and woman while subtracting man is equal to the vector associated with queen. In others words, subtracting the concept of man to the concept of King we get a representation of the \"royalty\". Then, if we sum to the woman word this concept we obtain the word \"queen\". Another example is: france – paris + rome = italy. In this case, the vector difference between paris and france captures the concept of country.\n",
    "\n",
    "Now we will show some of thise analogies in different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343cdf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman =  [('queen', 0.7698541283607483)]\n",
      "France - Paris + Rome =  [('italy', 0.8295993208885193)]\n",
      "France - french + english =  [('england', 0.7678162455558777)]\n",
      "December - November + June =  [('july', 0.9814670085906982)]\n",
      "Man - Woman + Sister =  [('brother', 0.8288711309432983)]\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print('King - Man + Woman = ',result)\n",
    "result = model.most_similar(positive=['rome', 'france'], negative=['paris'], topn=1)\n",
    "print('France - Paris + Rome = ',result)\n",
    "result = model.most_similar(positive=['english', 'france'], negative=['french'], topn=1)\n",
    "print('France - french + english = ',result)\n",
    "result = model.most_similar(positive=['june', 'december'], negative=['november'], topn=1)\n",
    "print('December - November + June = ',result)\n",
    "result = model.most_similar(positive=['sister', 'man'], negative=['woman'], topn=1)\n",
    "print('Man - Woman + Sister = ',result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d0ad5",
   "metadata": {},
   "source": [
    "We can observe how the word vectors include information to relate countries with nationalities, months of the year, family relationships, etc.\n",
    "\n",
    "But not always we get the expected results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16d8b3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France - Paris + Rome =  [('uncle', 0.8936851620674133)]\n"
     ]
    }
   ],
   "source": [
    "#But not always we get the expected result\n",
    "result = model.most_similar(positive=['aunt', 'nephew'], negative=['niece'], topn=1)\n",
    "print('France - Paris + Rome = ',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db40126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
