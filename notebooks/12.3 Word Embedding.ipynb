{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eea016e",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "Neural networks and ANNs have been shown to be very powerful for many natural langauge processing tasks.  In this notebook, we first describe how to preprocess words into numeric inputs that neural networks can understand.  We will then describe embedding as a technique not only to convert words and sentences into numbers, but also as a generic dimensionality reduction technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb651fd",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "Let's first understand how to preprocess textual data for our neural networks. Neural networks can take numbers as an input, but not raw text. Hence, we need to figure out a way to convert these words into a numerical format. The traditional method we used to follow is one hot encoding.\n",
    "\n",
    "In one hot encoding, we essentially represent each word as a vector of length V, where V is the total number of unique words available in the entire textual data. V is also called the vocabulary count. Each word’s one hot encoded vector is essentially a binary vector with the value 1 being in a unique index for each word and the value 0 being in every other index of the vector. \n",
    "\n",
    "Let's create some text to understand One-Hot Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dba7837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    'A wonderful book',\n",
    "    'Love the book',\n",
    "    'Pulitzer level quality',\n",
    "    'A beach reading',\n",
    "    'Writing is awesome',\n",
    "    'Horrid ... never read again',\n",
    "    'Has lots of mistakes',\n",
    "    'Book is fine, bad delivery',\n",
    "    'A 5th grader job',\n",
    "    'Bad bad bad'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26011250",
   "metadata": {},
   "source": [
    "We will take vocabulary size as 50 and one-hot encode the words using one_hot function from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb586e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded reviews: [[33, 9, 2], [43, 38, 2], [2, 10, 24], [33, 11, 46], [8, 9, 7], [18, 31, 25, 45], [26, 31, 13, 24], [2, 9, 16, 26, 13], [33, 41, 39, 28], [26, 26, 26]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "\n",
    "VOCAB_SIZE = 50\n",
    "encoded_reviews = [one_hot(d,VOCAB_SIZE) for d in reviews]\n",
    "print(f'encoded reviews: {encoded_reviews}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395bed4e",
   "metadata": {},
   "source": [
    "One-hot encoding creates 2 problems for machine learning:\n",
    "\n",
    "- The size of each word’s one hot encoded vector will always be V, which is the size of the entire vocabulary. In the example it was 5 but usually the value of V can reach 10k or even 100k. This means we need a huge vector just to represent a single word, which can lead to excessive memory usage while representing text as vectors.\n",
    "- The index which is assigned to each word does not hold any semantic meaning, it is merely an arbitrary value assigned to it. When we consider the vectors for two words, we would ideally want the vectors of similar meaning to have similar vectors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c652c990",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "Word embedding (or word vector) is essentially a much more efficient method of representing word(s). Word embedding takes up much lesser space than one hot encoded vectors and they also hold semantic information regarding the word. \n",
    "\n",
    "The intuition behind embedding is this principle\n",
    "\n",
    "> Similar words occur more frequently together than dissimilar words.\n",
    "\n",
    "When a word occurs within the vicinity of another word, it doesn’t always mean it has a similar meaning, but when we consider the frequency of words which are found close together, we find that words of similar meaning are often found together.\n",
    "\n",
    "For example, the word “Dog” will be found within the vicinity of the word “Cat” a lot more frequently than it will be found within the vicinity of the word “Computer”, this is because “Dog” shares certain semantic similarity with “Cat” and there will hence be many sentences which have both “Dog” and “Cat”. This is the key factor which deep learning researchers have exploited to come up with word vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71de70c4",
   "metadata": {},
   "source": [
    "## Creating Word Vectors with Embedding \n",
    "Embedding layer enables us to convert each word into a fixed length vector of defined size. The resultant vector is a dense one with having real values instead of just 0’s and 1’s. The fixed length of word vectors helps us to represent words in a better way along with reduced dimensions.\n",
    "\n",
    "This way embedding layer works like a lookup table. The words are the keys in this table, while the dense word vectors are the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6281bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98f1bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Embedding,Dense\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae8424",
   "metadata": {},
   "source": [
    "Let's create a simple model with just 1 embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2445b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(input_dim=10,output_dim=4,input_length=2)\n",
    "model.add(embedding_layer)\n",
    "model.compile('adam','mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0adcc75",
   "metadata": {},
   "source": [
    "There are three parameters to the embedding layer\n",
    "\n",
    "- input_dim : Size of the vocabulary\n",
    "- output_dim : Length of the vector for each word\n",
    "- input_length : Maximum length of a sequence\n",
    "\n",
    "In the above example, we are setting 10 as the vocabulary size, as we will be encoding numbers 0 to 9. We want the length of the word vector to be 4, hence output_dim is set to 4. The length of the input sequence to embedding layer will be 2.\n",
    "\n",
    "Let's give this a go with sample inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "596933a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "(1, 10)\n",
      "[[[-0.02180538 -0.01846747  0.00881319 -0.04496161]\n",
      "  [-0.02187794  0.01439232  0.03302777 -0.01083774]\n",
      "  [-0.01575593 -0.04280739  0.04219476 -0.00234377]\n",
      "  [-0.02075353 -0.02370724  0.01560435 -0.03962264]\n",
      "  [-0.00951229  0.02094544 -0.04710805  0.04518663]\n",
      "  [-0.04701214  0.01733999 -0.04683822  0.04251542]\n",
      "  [-0.03966779  0.02537152 -0.01777736  0.04978364]\n",
      "  [-0.04238213 -0.00793669  0.0344539  -0.02890325]\n",
      "  [-0.02519262  0.04823616  0.01561889 -0.03657982]\n",
      "  [ 0.00447683  0.0488834   0.02157776  0.03869686]]]\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "pred = model.predict(input_data)\n",
    "print(input_data.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c1964b",
   "metadata": {},
   "source": [
    "The \"embedding\" vectors are initialized by Keras but not _trained__ yet.  Let's first talk about what they means first.\n",
    "\n",
    "These weights are the \"embedding\" vector representations of the \"words\" in vocabulary {0, 1, ... 9}.  This is a lookup table of size 10 x 4, for words 0 to 9. The first word (0) is represented by first row in this table, which is:\n",
    "\n",
    "```[-0.02180538 -0.01846747  0.00881319 -0.04496161]```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e839a42",
   "metadata": {},
   "source": [
    "## Training Word Embedding with Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c8033",
   "metadata": {},
   "source": [
    "Let's create some sample book reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21307359",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    'A wonderful book',\n",
    "    'Love the book',\n",
    "    'Pulitzer level quality',\n",
    "    'A beach reading',\n",
    "    'Writing is awesome',\n",
    "    'Horrid ... never read again',\n",
    "    'Has lots of mistakes',\n",
    "    'Book is fine, bad delivery',\n",
    "    'A 5th grader job',\n",
    "    'Bad bad bad'\n",
    "]\n",
    "\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a3cd31",
   "metadata": {},
   "source": [
    "We will take vocabulary size as 50 and one-hot encode the words using one_hot function from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d463057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded reviews: [[33, 9, 2], [43, 38, 2], [2, 10, 24], [33, 11, 46], [8, 9, 7], [18, 31, 25, 45], [26, 31, 13, 24], [2, 9, 16, 26, 13], [33, 41, 39, 28], [26, 26, 26]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "\n",
    "VOCAB_SIZE = 50\n",
    "encoded_reviews = [one_hot(d,VOCAB_SIZE) for d in reviews]\n",
    "print(f'encoded reviews: {encoded_reviews}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c90ca1",
   "metadata": {},
   "source": [
    "Here you can see the length of each encoded review is equal to the number of words in that review. Keras one_hot is basically converting each word into its one-hot encoded index. Now we need to apply padding so that all the encoded reviews are of same length. Let’s define 5 as the maximum length and pad the encoded vectors with 0’s in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c16c2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33  9  2  0  0]\n",
      " [43 38  2  0  0]\n",
      " [ 2 10 24  0  0]\n",
      " [33 11 46  0  0]\n",
      " [ 8  9  7  0  0]\n",
      " [18 31 48 25 45]\n",
      " [26 31 13 24  0]\n",
      " [47 13 21 46 17]\n",
      " [32 46 33  5 28]\n",
      " [26 26 26  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_length = 5\n",
    "padded_reviews = pad_sequences(encoded_reviews,maxlen=max_length,padding='post')\n",
    "print(padded_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220cdc8a",
   "metadata": {},
   "source": [
    "After creating padded one-hot representation of the reviews, we are ready to pass it as input to the embedding layer. Let's create a simple Keras model. We will fix the length of embedded vectors for each word to be 8 and the input length will be the maximum length which we have already defined as 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bcd1732",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(input_dim=VOCAB_SIZE,output_dim=8,input_length=max_length)\n",
    "model.add(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6017cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 5, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25e0e368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 40)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add(Flatten())\n",
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04f9e013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "648eb74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 5, 8)              400       \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 40)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 441\n",
      "Trainable params: 441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b9ceb6",
   "metadata": {},
   "source": [
    "Let's train the model for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cb94896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f90010be640>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_reviews,labels,epochs=100,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd810176",
   "metadata": {},
   "source": [
    "Once the training is completed, embedding layer has learnt the weights which are nothing but the vector representations of each word. Lets check the shape of the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "affbed8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 8)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.get_weights()[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d6fdae",
   "metadata": {},
   "source": [
    "If we check the embeddings for the first word, we get the following vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "908653b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02152041 -0.02873616 -0.02618781  0.03694941 -0.03886317 -0.04280834\n",
      "  0.00144855 -0.04730001]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.get_weights()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705392b7",
   "metadata": {},
   "source": [
    "So this is how we train an embedding layer on our text corpus and get the embedded vectors for each word. These vectors are then used to represent words in a sentence.\n",
    "\n",
    "Embeddings are a great way to deal with NLP problems because of two reasons\n",
    "- it helps in dimensionality reduction over one-hot encoding as we can control the number of features\n",
    "- it is capable of understanding the context of a word so that similar words have similar embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
