{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b90848f",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "We will be using NLTK library to illustrate basic text processing functionalities: tokenization, lemmization, stop words, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a89c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbbe243",
   "metadata": {},
   "source": [
    "## Text Corpora\n",
    "\n",
    "A text corpus is a large body of text. Many corpora are designed to contain a careful balance of material in one or more genres.  \n",
    "\n",
    "Let's start by loading the Gutenberg corpora.  The Project Gutenberg corpora is electronic text archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/. We begin by querying to see nltk.corpus.gutenberg.fileids(), the file identifiers in this corpus:\n",
    "\n",
    "[Reference](https://www.sketchengine.eu/gutenberg-corpora-2020/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab1f2fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /Users/pmui/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/pmui/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c54e3c",
   "metadata": {},
   "source": [
    "The first text is Emma by Jane Austen.  How many words does it contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cafd92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = gutenberg.words('austen-emma.txt')\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd5a94",
   "metadata": {},
   "source": [
    "Let's print out all info about the gutenberg corpora by looping over all the values of fileid corresponding to the gutenberg file identifiers listed earlier and then computing statistics for each text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b942878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars/word, words/sent, words/vocab\n",
      "5 25 26 austen-emma.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "5 26 17 austen-persuasion.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "5 28 22 austen-sense.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "4 34 79 bible-kjv.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "5 19 5 blake-poems.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "4 19 14 bryant-stories.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "4 18 12 burgess-busterbrown.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "4 20 13 carroll-alice.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "5 20 12 chesterton-ball.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "5 23 11 chesterton-brown.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "5 19 11 chesterton-thursday.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "4 21 25 edgeworth-parents.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "5 26 15 melville-moby_dick.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "5 52 11 milton-paradise.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "4 12 9 shakespeare-caesar.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "4 12 8 shakespeare-hamlet.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "4 12 7 shakespeare-macbeth.txt\n",
      "chars/word, words/sent, words/vocab\n",
      "5 36 12 whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "    print(\"chars/word, words/sent, words/vocab\")\n",
    "    print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b39c9",
   "metadata": {},
   "source": [
    "The raw() function gives us the contents of the file without any linguistic processing. So, for example, len(gutenberg.raw('blake-poems.txt')) tells us how many letters occur in the text, including the spaces between words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63b83db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38153"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gutenberg.raw('blake-poems.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba8095",
   "metadata": {},
   "source": [
    "The sents() function divides the text up into its sentences, where each sentence is a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "709894f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']'], ['Actus', 'Primus', '.'], ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "macbeth_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdc52e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Double',\n",
       " ',',\n",
       " 'double',\n",
       " ',',\n",
       " 'toile',\n",
       " 'and',\n",
       " 'trouble',\n",
       " ';',\n",
       " 'Fire',\n",
       " 'burne',\n",
       " ',',\n",
       " 'and',\n",
       " 'Cauldron',\n",
       " 'bubble']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_sentences[1116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4430696f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_len = max(len(s) for s in macbeth_sentences)\n",
    "longest_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58f9e233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Doubtfull',\n",
       "  'it',\n",
       "  'stood',\n",
       "  ',',\n",
       "  'As',\n",
       "  'two',\n",
       "  'spent',\n",
       "  'Swimmers',\n",
       "  ',',\n",
       "  'that',\n",
       "  'doe',\n",
       "  'cling',\n",
       "  'together',\n",
       "  ',',\n",
       "  'And',\n",
       "  'choake',\n",
       "  'their',\n",
       "  'Art',\n",
       "  ':',\n",
       "  'The',\n",
       "  'mercilesse',\n",
       "  'Macdonwald',\n",
       "  '(',\n",
       "  'Worthie',\n",
       "  'to',\n",
       "  'be',\n",
       "  'a',\n",
       "  'Rebell',\n",
       "  ',',\n",
       "  'for',\n",
       "  'to',\n",
       "  'that',\n",
       "  'The',\n",
       "  'multiplying',\n",
       "  'Villanies',\n",
       "  'of',\n",
       "  'Nature',\n",
       "  'Doe',\n",
       "  'swarme',\n",
       "  'vpon',\n",
       "  'him',\n",
       "  ')',\n",
       "  'from',\n",
       "  'the',\n",
       "  'Westerne',\n",
       "  'Isles',\n",
       "  'Of',\n",
       "  'Kernes',\n",
       "  'and',\n",
       "  'Gallowgrosses',\n",
       "  'is',\n",
       "  'supply',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  ',',\n",
       "  'And',\n",
       "  'Fortune',\n",
       "  'on',\n",
       "  'his',\n",
       "  'damned',\n",
       "  'Quarry',\n",
       "  'smiling',\n",
       "  ',',\n",
       "  'Shew',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'like',\n",
       "  'a',\n",
       "  'Rebells',\n",
       "  'Whore',\n",
       "  ':',\n",
       "  'but',\n",
       "  'all',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'too',\n",
       "  'weake',\n",
       "  ':',\n",
       "  'For',\n",
       "  'braue',\n",
       "  'Macbeth',\n",
       "  '(',\n",
       "  'well',\n",
       "  'hee',\n",
       "  'deserues',\n",
       "  'that',\n",
       "  'Name',\n",
       "  ')',\n",
       "  'Disdayning',\n",
       "  'Fortune',\n",
       "  ',',\n",
       "  'with',\n",
       "  'his',\n",
       "  'brandisht',\n",
       "  'Steele',\n",
       "  ',',\n",
       "  'Which',\n",
       "  'smoak',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'with',\n",
       "  'bloody',\n",
       "  'execution',\n",
       "  '(',\n",
       "  'Like',\n",
       "  'Valours',\n",
       "  'Minion',\n",
       "  ')',\n",
       "  'caru',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'out',\n",
       "  'his',\n",
       "  'passage',\n",
       "  ',',\n",
       "  'Till',\n",
       "  'hee',\n",
       "  'fac',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'the',\n",
       "  'Slaue',\n",
       "  ':',\n",
       "  'Which',\n",
       "  'neu',\n",
       "  \"'\",\n",
       "  'r',\n",
       "  'shooke',\n",
       "  'hands',\n",
       "  ',',\n",
       "  'nor',\n",
       "  'bad',\n",
       "  'farwell',\n",
       "  'to',\n",
       "  'him',\n",
       "  ',',\n",
       "  'Till',\n",
       "  'he',\n",
       "  'vnseam',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'him',\n",
       "  'from',\n",
       "  'the',\n",
       "  'Naue',\n",
       "  'toth',\n",
       "  \"'\",\n",
       "  'Chops',\n",
       "  ',',\n",
       "  'And',\n",
       "  'fix',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'his',\n",
       "  'Head',\n",
       "  'vpon',\n",
       "  'our',\n",
       "  'Battlements']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in macbeth_sentences if len(s) == longest_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697108ce",
   "metadata": {},
   "source": [
    "## Brown Corpus\n",
    "\n",
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, etc.  A complete list of genres for the Brown Corpus can be found at: http://icame.uib.no/brown/bcm-los.html.\n",
    "\n",
    "We can access the corpus as a list of words, or a list of sentences (where each sentence is itself just a list of words). We can optionally specify particular categories or files to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e994a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dbc1e8",
   "metadata": {},
   "source": [
    "The Brown Corpus is a convenient resource for studying systematic differences between genres, a kind of linguistic inquiry known as stylistics. Let's compare genres in their usage of modal verbs. The first step is to produce the counts for a particular genre. Remember to import nltk before doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bb6360d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 94 could: 87 may: 93 might: 38 must: 53 will: 389 "
     ]
    }
   ],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "\n",
    "# let's find the frequency of words within a text\n",
    "news_dist = nltk.FreqDist(w.lower() for w in news_text)\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "for m in modals:\n",
    "    print(m + ':', news_dist[m], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "106cc6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what: 95 when: 169 where: 59 who: 268 why: 14 "
     ]
    }
   ],
   "source": [
    "five_w = ['what', 'when', 'where', 'who', 'why']\n",
    "for f in five_w:\n",
    "    print(f + ':', news_dist[f], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be12fd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 39 could: 168 may: 10 might: 44 must: 55 will: 56 "
     ]
    }
   ],
   "source": [
    "fiction_text = brown.words(categories='fiction')\n",
    "fiction_dist = nltk.FreqDist(w.lower() for w in fiction_text)\n",
    "for m in modals:\n",
    "    print(m + ':', fiction_dist[m], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1ebb38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what: 186 when: 192 where: 89 who: 112 why: 42 "
     ]
    }
   ],
   "source": [
    "for f in five_w:\n",
    "    print(f + ':', fiction_dist[f], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d3315",
   "metadata": {},
   "source": [
    "We would like to obtain counts for each genre of interest. We'll use NLTK's support for conditional frequency distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbc7370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((genre, word)\n",
    "                                for genre in brown.categories()\n",
    "                                for word in brown.words(categories=genre))\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5adf9a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  can could   may might  must  will \n",
      "           news    93    86    66    38    50   389 \n",
      "       religion    82    59    78    12    54    71 \n",
      "        hobbies   268    58   131    22    83   264 \n",
      "science_fiction    16    49     4    12     8    16 \n",
      "        romance    74   193    11    51    45    43 \n",
      "          humor    16    30     8     8     9    13 \n"
     ]
    }
   ],
   "source": [
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c52a47fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 what  when where   who   why \n",
      "           news    76   128    58   268     9 \n",
      "       religion    64    53    20   100    14 \n",
      "        hobbies    78   119    72   103    10 \n",
      "science_fiction    27    21    10    13     4 \n",
      "        romance   121   126    54    89    34 \n",
      "          humor    36    52    15    48     9 \n"
     ]
    }
   ],
   "source": [
    "cfd.tabulate(conditions=genres, samples=five_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f83942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
